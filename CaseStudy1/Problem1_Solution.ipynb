{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.parsers import TextParser\n",
    "from lxml.html import parse \n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/5114sss3/000005114313000007/0000051143-13-000007-index.html'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1 = \"\"\n",
    "url_pre = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "def geturl(cik, accno):\n",
    "    cik_striped = cik.lstrip(\"0\")\n",
    "    accno_striped = accno.replace(\"-\",\"\")\n",
    "    url1 = url_pre + cik_striped + \"/\" + accno_striped + \"/\" + accno + \"-index.html\"\n",
    "    return url1\n",
    "\n",
    "geturl(\"000005114sss3\", \"0000051143-13-000007\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Q\n",
      "<td scope=\"row\">10-Q</td>\n"
     ]
    }
   ],
   "source": [
    "# retrieve the web page\n",
    "page = urllib.request.urlopen(geturl(\"0000051143\", \"0000051143-13-000007\"))\n",
    "# parse the page and save it in soup format \n",
    "soup = BeautifulSoup(page,\"lxml\")\n",
    "#print(soup.prettify)\n",
    "url2 = \"\"\n",
    "\n",
    "form = soup.find(id='formName').get_text()\n",
    "formname = form[6:10]\n",
    "print(formname)\n",
    "\n",
    "#formtype = soup.find(text=formname)\n",
    "formtype = soup.findAll('td', text = formname)[0]\n",
    "\n",
    "formprevious = soup.find_previous_sibling('td', formtype)\n",
    "\n",
    "print(formtype)\n",
    "\n",
    "#all_links = soup.find_all('a')\n",
    "#for link in all_links:\n",
    "#    if \"10q.htm\" in link.get(\"href\"):\n",
    "#        url2 = \"https://www.sec.gov/\" + link.get(\"href\")\n",
    "        \n",
    "#print(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov//Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\n"
     ]
    }
   ],
   "source": [
    "# retrieve the web page\n",
    "print(url2)\n",
    "page_10q = urllib.request.urlopen(url2)\n",
    "soup = BeautifulSoup(page_10q,\"lxml\")\n",
    "#print(soup.prettify)\n",
    "#parsed = parse(urlopen(url2))\n",
    "\n",
    "#doc = parsed.getroot()\n",
    "#all_tables=doc.findall('.//table')\n",
    "#all_tables = all_tables[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tables= soup.select('div[\"bclpageborder\"] table')\n",
    "#all_tables = soup.findAll('table')\n",
    "for tab in all_tables:\n",
    "    records = []\n",
    "    for tr in tab.find_all('tr'):\n",
    "        rowString=[]\n",
    "        for td in tr.findAll('td'):\n",
    "            p = td.find_all('p')\n",
    "            if len(p)>0:\n",
    "                for ps in p:\n",
    "                    ps_text = ps.get_text().replace(\"\\n\",\" \") \n",
    "                    ps_text = ps_text.replace(\"\\xa0\",\"\")                 \n",
    "                    rowString.append(ps_text)\n",
    "            else:\n",
    "                td_text=td.get_text().replace(\"\\n\",\" \")\n",
    "                td_text = td_text.replace(\"\\xa0\",\"\")\n",
    "                rowString.append(td_text)\n",
    "        records.append(rowString)        \n",
    "    with open(str(all_tables.index(tab)) + 'tables.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(records)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
