{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### Import Libraries ###############\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging # for logging\n",
    "import sys\n",
    "import shutil #to delete the directory contents\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path = os.path.join(os.path.dirname(__file__),'downloaded_zips_unzipped')\n",
    "#filelist = glob.glob('downloaded_zips_unzipped' + \"/*.csv\")\n",
    "#read them into pandas\n",
    "#df_list = [pd.read_csv(file) for file in filelist]\n",
    "#all_csv_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### Initializing logging file ###############\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "ch1 = logging.FileHandler('problem2_log.log') #output the logs to a file\n",
    "ch1.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch1.setFormatter(formatter)\n",
    "root.addHandler(ch1)\n",
    "\n",
    "ch = logging.StreamHandler(sys.stdout ) #print the logs in console as well\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ef4ca26d5e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'downloaded_zips'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'downloaded_zips'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'downloaded_zips'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "############### Cleanup required directories ###############\n",
    "if not os.path.exists('downloaded_zips'):\n",
    "    os.makedirs('downloaded_zips', mode=0o777)\n",
    "else:\n",
    "    shutil.rmtree(os.path.join(os.path.dirname(__file__),'downloaded_zips'), ignore_errors=False)\n",
    "    os.makedirs('downloaded_zips', mode=0o777)\n",
    "\n",
    "if not os.path.exists('downloaded_zips_unzipped'):\n",
    "    os.makedirs('downloaded_zips_unzipped', mode=0o777)\n",
    "else:\n",
    "    shutil.rmtree(os.path.join(os.path.dirname(__file__), 'downloaded_zips_unzipped'), ignore_errors=False)\n",
    "    os.makedirs('downloaded_zips_unzipped', mode=0o777)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_zip(url):\n",
    "    zips = []\n",
    "    try:\n",
    "        zips.append(urllib.request.urlretrieve(url, filename= 'downloaded_zips/'+url[-15:]))\n",
    "        if os.path.getsize('downloaded_zips/'+url[-15:]) <= 4515: #catching empty file\n",
    "            os.remove('downloaded_zips/'+url[-15:])\n",
    "            logging.warning('Log file %s is empty. Attempting to download for next date.', url[-15:])\n",
    "            return False\n",
    "        else:\n",
    "            logging.info('Log file %s successfully downloaded', url[-15:])\n",
    "            return True\n",
    "    except Exception as e: #Catching file note found\n",
    "        #print(str(e))\n",
    "        logging.warning('Log %s not found...Skipping ahead!', url[-15:])\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the year for which you want to analyze the logs:\n",
      "WARNING - Program running for 2003 by default since you did not enter any Year.\n",
      "WARNING - Program running for 2003 by default since you did not enter any Year.\n",
      "WARNING - Log file log20030101.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030101.zip is empty. Attempting to download for next date.\n",
      "INFO - Log file log20030102.zip successfully downloaded\n",
      "INFO - Log file log20030102.zip successfully downloaded\n",
      "WARNING - Log file log20030201.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030201.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030202.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030202.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030203.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030203.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030204.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030204.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030205.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030205.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030206.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030206.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030207.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030207.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030208.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030208.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030209.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030209.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030210.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030210.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030211.zip is empty. Attempting to download for next date.\n",
      "WARNING - Log file log20030211.zip is empty. Attempting to download for next date.\n",
      "INFO - Log file log20030212.zip successfully downloaded\n",
      "INFO - Log file log20030212.zip successfully downloaded\n",
      "INFO - Log file log20030301.zip successfully downloaded\n",
      "INFO - Log file log20030301.zip successfully downloaded\n",
      "INFO - Log file log20031001.zip successfully downloaded\n",
      "INFO - Log file log20031001.zip successfully downloaded\n",
      "INFO - Log file log20031101.zip successfully downloaded\n",
      "INFO - Log file log20031101.zip successfully downloaded\n",
      "INFO - Log file log20031201.zip successfully downloaded\n",
      "INFO - Log file log20031201.zip successfully downloaded\n",
      "INFO - Log file log20030401.zip successfully downloaded\n",
      "INFO - Log file log20030401.zip successfully downloaded\n",
      "INFO - Log file log20030501.zip successfully downloaded\n",
      "INFO - Log file log20030501.zip successfully downloaded\n",
      "INFO - Log file log20030601.zip successfully downloaded\n",
      "INFO - Log file log20030601.zip successfully downloaded\n",
      "INFO - Log file log20030701.zip successfully downloaded\n",
      "INFO - Log file log20030701.zip successfully downloaded\n",
      "INFO - Log file log20030801.zip successfully downloaded\n",
      "INFO - Log file log20030801.zip successfully downloaded\n",
      "INFO - Log file log20030901.zip successfully downloaded\n",
      "INFO - Log file log20030901.zip successfully downloaded\n",
      "INFO - All log files downloaded\n",
      "INFO - All log files downloaded\n"
     ]
    }
   ],
   "source": [
    "############### Generate URLs for the inputted year ###############\n",
    "#http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2016/Qtr1/log20160101.zip\n",
    "url_pre = \"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"\n",
    "qtr_months = {'Qtr1':['01','02','03'], 'Qtr2':['04','05','06'], 'Qtr3':['07','08','09'], 'Qtr4':['10','11','12']}\n",
    "year = input(\"Please enter the year for which you want to analyze the logs:\")\n",
    "valid_years = range(2003,2017)\n",
    "days = range(1,32)\n",
    "\n",
    "if not year:\n",
    "    year = 2003\n",
    "    logging.warning('Program running for 2003 by default since you did not enter any Year.')\n",
    "\n",
    "if int(year) not in valid_years:\n",
    "    logging.error(\"Invalid year. Please enter a valid year between 2003 and 2016.\")\n",
    "    exit()\n",
    "\n",
    "url_final = []\n",
    "for key, val in qtr_months.items():\n",
    "    for v in val:\n",
    "        for d in days:\n",
    "            url = url_pre +str(year) +'/' +str(key) +'/' +'log' +str(year) +str(v) + str(format(d,'02d')) +'.zip'\n",
    "            if download_zip(url):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "logging.info('All log files downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Zip files successfully extracted to folder: downloaded_zips_unzipped.\n",
      "INFO - Zip files successfully extracted to folder: downloaded_zips_unzipped.\n"
     ]
    }
   ],
   "source": [
    "############### Unzip the logs and extract csv ###############\n",
    "try:\n",
    "    zip_files = os.listdir('downloaded_zips')\n",
    "    for f in zip_files:\n",
    "        z = zipfile.ZipFile(os.path.join('downloaded_zips', f), 'r')\n",
    "        for file in z.namelist():\n",
    "            if file.endswith('.csv'):\n",
    "                z.extract(file, r'downloaded_zips_unzipped')\n",
    "    logging.info('Zip files successfully extracted to folder: downloaded_zips_unzipped.')\n",
    "except Exception as e:\n",
    "        logging.error(str(e))\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### Load the csvs into dataframe ###############\n",
    "filelists = glob.glob('downloaded_zips_unzipped' + \"/*.csv\")\n",
    "#url = '{}'\n",
    "all_csv_df_dict = {period: pd.read_csv(period) for period in filelists}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = all_csv_df_dict['downloaded_zips_unzipped\\\\log20030301.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### HANDLING MISSING VALUES for one dataframe at a time ###############\n",
    "for key, val in all_csv_df_dict.items():\n",
    "    df = all_csv_df_dict[key]\n",
    "    #remove rows which have no ip, cik or accession\n",
    "    df.dropna(subset=['cik'])\n",
    "    df.dropna(subset=['accession'])\n",
    "    df.dropna(subset=['ip'])\n",
    "    df.dropna(subset=['date'])\n",
    "    df.dropna(subset=['time'])\n",
    "    #replace nan with the most used browser in data.\n",
    "    max_browser = pd.DataFrame(df.groupby('browser').size().rename('cnt')).idxmax()[0]\n",
    "    df['browser'] = df['browser'].fillna(max_browser)\n",
    "    # replace nan idx with max idx\n",
    "    max_idx = pd.DataFrame(df.groupby('idx').size().rename('cnt')).idxmax()[0]\n",
    "    df['idx'] = df['idx'].fillna(max_idx)\n",
    "    # replace nan code with max code\n",
    "    max_code = pd.DataFrame(df.groupby('code').size().rename('cnt')).idxmax()[0]\n",
    "    df['code'] = df['code'].fillna(max_code)\n",
    "    # replace nan norefer with zero\n",
    "    df['norefer'] = df['norefer'].fillna('0')\n",
    "    # replace nan noagent with zero\n",
    "    df['noagent'] = df['noagent'].fillna('0')\n",
    "    # replace nan find with max find\n",
    "    max_find = pd.DataFrame(df.groupby('find').size().rename('cnt')).idxmax()[0]\n",
    "    df['find'] = df['find'].fillna(max_find)\n",
    "    # replace nan crawler with zero\n",
    "    df['crawler'] = df['crawler'].fillna('0')\n",
    "    # replace nan extention with max extention\n",
    "    max_extention = pd.DataFrame(df.groupby('extention').size().rename('cnt')).idxmax()[0]\n",
    "    df['extention'] = df['extention'].fillna(max_extention)\n",
    "    # replace nan extention with max extention\n",
    "    max_zone = pd.DataFrame(df.groupby('zone').size().rename('cnt')).idxmax()[0]\n",
    "    df['zone'] = df['zone'].fillna(max_zone)\n",
    "    # find mean of the size and replace null values with the mean\n",
    "    size_mean = df['size'].mean(axis=0) #mean of size\n",
    "    df['size'] = df['size'].fillna(size_mean)\n",
    "    df['idx'] = df['idx'].fillna(df['idx'].max(axis=0))\n",
    "    \n",
    "    #Compute mean size\n",
    "    df['size_mean'] = df['size'].mean(axis=0)\n",
    "    #Compute maximum used browser\n",
    "    df['max_browser'] = pd.DataFrame(df.groupby('browser').size().rename('cnt')).idxmax()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### Combining dataframe and write to a csv ###############\n",
    "master_df = pd.concat(all_csv_df_dict)\n",
    "master_df.to_csv('master_csv.csv')\n",
    "# write csv for summary of combined data.\n",
    "master_df_summary = master_df.describe()\n",
    "master_df_summary.to_csv('master_df_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### HANDLING MISSING VALUES ############### OLD\n",
    "#replace nan with the most used browser in data.\n",
    "max_browser = pd.DataFrame(df.groupby('browser').size().rename('cnt')).idxmax()[0]\n",
    "df['browser'] = df['browser'].fillna(max_browser)\n",
    "\n",
    "# replace nan idx with max idx\n",
    "max_idx = pd.DataFrame(df.groupby('idx').size().rename('cnt')).idxmax()[0]\n",
    "df['idx'] = df['idx'].fillna(max_idx)\n",
    "\n",
    "# replace nan code with max code\n",
    "max_code = pd.DataFrame(df.groupby('code').size().rename('cnt')).idxmax()[0]\n",
    "df['code'] = df['code'].fillna(max_code)\n",
    "\n",
    "# replace nan norefer with zero\n",
    "df['norefer'] = df['norefer'].fillna('0')\n",
    "\n",
    "# replace nan noagent with zero\n",
    "df['noagent'] = df['noagent'].fillna('0')\n",
    "\n",
    "# replace nan find with max find\n",
    "max_find = pd.DataFrame(df.groupby('find').size().rename('cnt')).idxmax()[0]\n",
    "df['find'] = df['find'].fillna(max_find)\n",
    "\n",
    "# replace nan crawler with zero\n",
    "df['crawler'] = df['crawler'].fillna('0')\n",
    "\n",
    "# replace nan extention with max extention\n",
    "max_extention = pd.DataFrame(df.groupby('extention').size().rename('cnt')).idxmax()[0]\n",
    "df['extention'] = df['extention'].fillna(max_extention)\n",
    "\n",
    "# replace nan extention with max extention\n",
    "max_zone = pd.DataFrame(df.groupby('zone').size().rename('cnt')).idxmax()[0]\n",
    "df['zone'] = df['zone'].fillna(max_zone)\n",
    "\n",
    "\n",
    "# find mean of the size and replace null values with the mean\n",
    "size_mean = df['size'].mean(axis=0) #mean of size\n",
    "df['size'] = df['size'].fillna(size_mean)\n",
    "df['idx'] = df['idx'].fillna(df['idx'].max(axis=0))\n",
    "\n",
    "#remove rows which have no ip, cik or accession\n",
    "df.dropna(subset=['cik'])\n",
    "df.dropna(subset=['accession'])\n",
    "df.dropna(subset=['ip'])\n",
    "df.dropna(subset=['date'])\n",
    "df.dropna(subset=['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### SUMMARY METRICS ###############\n",
    "# Calculating Summary metrics\n",
    "df.describe() # output this\n",
    "\n",
    "#Compute mean size\n",
    "df['size_mean'] = df['size'].mean(axis=0)\n",
    "\n",
    "#Compute maximum used browser\n",
    "df['max_browser'] = pd.DataFrame(df.groupby('browser').size().rename('cnt')).idxmax()[0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
